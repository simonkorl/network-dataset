{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"It's my great pleasure to be here today.\",\n",
    "    \"\"\"\n",
    "    Sometimes I have thought it would be an excellent rule to live each day as if we should die tomorrow. \n",
    "    Such an attitude would emphasize sharply the values of life. We should live each day with gentleness, \n",
    "    vigor and a keenness of appreciation which are often lost when time stretches before us in the constant \n",
    "    panorama of more days and months and years to come. There are those, of course, \n",
    "    who would adopt the Epicurean motto of “Eat, drink, and be merry”. \n",
    "    But most people would be chastened by the certainty of impending death.\n",
    "    \"\"\",\n",
    "    \"这是一段中文语料，我希望它能够被分词。\",\n",
    "    \"1、如是本人使用的号码，可凭户主有效证件到营业厅查询实名信息；2、如不是本人的号码，因涉及到个人隐私，无法查询到户主的个人信息。\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'my', 'great', 'pleasure', 'to', 'be', 'here', 'today', '.']\n",
      "['Sometimes', 'I', 'have', 'thought', 'it', 'would', 'be', 'an', 'excellent', 'rule', 'to', 'live', 'each', 'day', 'as', 'if', 'we', 'should', 'die', 'tomorrow', '.', 'Such', 'an', 'attitude', 'would', 'emphasize', 'sharply', 'the', 'values', 'of', 'life', '.', 'We', 'should', 'live', 'each', 'day', 'with', 'gentleness', ',', 'vigor', 'and', 'a', 'keenness', 'of', 'appreciation', 'which', 'are', 'often', 'lost', 'when', 'time', 'stretches', 'before', 'us', 'in', 'the', 'constant', 'panorama', 'of', 'more', 'days', 'and', 'months', 'and', 'years', 'to', 'come', '.', 'There', 'are', 'those', ',', 'of', 'course', ',', 'who', 'would', 'adopt', 'the', 'Epicurean', 'motto', 'of', '“', 'Eat', ',', 'drink', ',', 'and', 'be', 'merry', '”', '.', 'But', 'most', 'people', 'would', 'be', 'chastened', 'by', 'the', 'certainty', 'of', 'impending', 'death', '.']\n",
      "['这是一段中文语料，我希望它能够被分词。']\n",
      "['1、如是本人使用的号码，可凭户主有效证件到营业厅查询实名信息；2、如不是本人的号码，因涉及到个人隐私，无法查询到户主的个人信息。']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 's', 'my', 'great', 'pleasure', 'to', 'be', 'here', 'today']\n",
      "['Sometimes', 'I', 'have', 'thought', 'it', 'would', 'be', 'an', 'excellent', 'rule', 'to', 'live', 'each', 'day', 'as', 'if', 'we', 'should', 'die', 'tomorrow', 'Such', 'an', 'attitude', 'would', 'emphasize', 'sharply', 'the', 'values', 'of', 'life', 'We', 'should', 'live', 'each', 'day', 'with', 'gentleness', 'vigor', 'and', 'a', 'keenness', 'of', 'appreciation', 'which', 'are', 'often', 'lost', 'when', 'time', 'stretches', 'before', 'us', 'in', 'the', 'constant', 'panorama', 'of', 'more', 'days', 'and', 'months', 'and', 'years', 'to', 'come', 'There', 'are', 'those', 'of', 'course', 'who', 'would', 'adopt', 'the', 'Epicurean', 'motto', 'of', 'Eat', 'drink', 'and', 'be', 'merry', 'But', 'most', 'people', 'would', 'be', 'chastened', 'by', 'the', 'certainty', 'of', 'impending', 'death']\n",
      "['这是一段中文语料', '我希望它能够被分词']\n",
      "['如是本人使用的号码', '可凭户主有效证件到营业厅查询实名信息', '如不是本人的号码', '因涉及到个人隐私', '无法查询到户主的个人信息']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = list(tokenize(sentence))\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 't', \"'\", 's', 'm', 'y', 'g', 'r', 'e', 'a', 't', 'p', 'l', 'e', 'a', 's', 'u', 'r', 'e', 't', 'o', 'b', 'e', 'h', 'e', 'r', 'e', 't', 'o', 'd', 'a', 'y', '.']\n",
      "['\\n    ', 'S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', 'I', 'h', 'a', 'v', 'e', 't', 'h', 'o', 'u', 'g', 'h', 't', 'i', 't', 'w', 'o', 'u', 'l', 'd', 'b', 'e', 'a', 'n', 'e', 'x', 'c', 'e', 'l', 'l', 'e', 'n', 't', 'r', 'u', 'l', 'e', 't', 'o', 'l', 'i', 'v', 'e', 'e', 'a', 'c', 'h', 'd', 'a', 'y', 'a', 's', 'i', 'f', 'w', 'e', 's', 'h', 'o', 'u', 'l', 'd', 'd', 'i', 'e', 't', 'o', 'm', 'o', 'r', 'r', 'o', 'w', '.', '\\n    ', 'S', 'u', 'c', 'h', 'a', 'n', 'a', 't', 't', 'i', 't', 'u', 'd', 'e', 'w', 'o', 'u', 'l', 'd', 'e', 'm', 'p', 'h', 'a', 's', 'i', 'z', 'e', 's', 'h', 'a', 'r', 'p', 'l', 'y', 't', 'h', 'e', 'v', 'a', 'l', 'u', 'e', 's', 'o', 'f', 'l', 'i', 'f', 'e', '.', 'W', 'e', 's', 'h', 'o', 'u', 'l', 'd', 'l', 'i', 'v', 'e', 'e', 'a', 'c', 'h', 'd', 'a', 'y', 'w', 'i', 't', 'h', 'g', 'e', 'n', 't', 'l', 'e', 'n', 'e', 's', 's', ',', '\\n    ', 'v', 'i', 'g', 'o', 'r', 'a', 'n', 'd', 'a', 'k', 'e', 'e', 'n', 'n', 'e', 's', 's', 'o', 'f', 'a', 'p', 'p', 'r', 'e', 'c', 'i', 'a', 't', 'i', 'o', 'n', 'w', 'h', 'i', 'c', 'h', 'a', 'r', 'e', 'o', 'f', 't', 'e', 'n', 'l', 'o', 's', 't', 'w', 'h', 'e', 'n', 't', 'i', 'm', 'e', 's', 't', 'r', 'e', 't', 'c', 'h', 'e', 's', 'b', 'e', 'f', 'o', 'r', 'e', 'u', 's', 'i', 'n', 't', 'h', 'e', 'c', 'o', 'n', 's', 't', 'a', 'n', 't', '\\n    ', 'p', 'a', 'n', 'o', 'r', 'a', 'm', 'a', 'o', 'f', 'm', 'o', 'r', 'e', 'd', 'a', 'y', 's', 'a', 'n', 'd', 'm', 'o', 'n', 't', 'h', 's', 'a', 'n', 'd', 'y', 'e', 'a', 'r', 's', 't', 'o', 'c', 'o', 'm', 'e', '.', 'T', 'h', 'e', 'r', 'e', 'a', 'r', 'e', 't', 'h', 'o', 's', 'e', ',', 'o', 'f', 'c', 'o', 'u', 'r', 's', 'e', ',', '\\n    ', 'w', 'h', 'o', 'w', 'o', 'u', 'l', 'd', 'a', 'd', 'o', 'p', 't', 't', 'h', 'e', 'E', 'p', 'i', 'c', 'u', 'r', 'e', 'a', 'n', 'm', 'o', 't', 't', 'o', 'o', 'f', '“', 'E', 'a', 't', ',', 'd', 'r', 'i', 'n', 'k', ',', 'a', 'n', 'd', 'b', 'e', 'm', 'e', 'r', 'r', 'y', '”', '.', '\\n    ', 'B', 'u', 't', 'm', 'o', 's', 't', 'p', 'e', 'o', 'p', 'l', 'e', 'w', 'o', 'u', 'l', 'd', 'b', 'e', 'c', 'h', 'a', 's', 't', 'e', 'n', 'e', 'd', 'b', 'y', 't', 'h', 'e', 'c', 'e', 'r', 't', 'a', 'i', 'n', 't', 'y', 'o', 'f', 'i', 'm', 'p', 'e', 'n', 'd', 'i', 'n', 'g', 'd', 'e', 'a', 't', 'h', '.', '\\n    ']\n",
      "['这', '是', '一', '段', '中', '文', '语', '料', '，', '我', '希', '望', '它', '能', '够', '被', '分', '词', '。']\n",
      "['1', '、', '如', '是', '本', '人', '使', '用', '的', '号', '码', '，', '可', '凭', '户', '主', '有', '效', '证', '件', '到', '营', '业', '厅', '查', '询', '实', '名', '信', '息', '；', '2', '、', '如', '不', '是', '本', '人', '的', '号', '码', '，', '因', '涉', '及', '到', '个', '人', '隐', '私', '，', '无', '法', '查', '询', '到', '户', '主', '的', '个', '人', '信', '息', '。']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "nlp = Chinese()\n",
    "# nlp = English()\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num token: 14\n",
      "['[UNK]', \"'\", 's', 'my', 'great', 'p', '##le', '##as', '##ure', 'to', 'be', 'here', 'today', '.']\n",
      "num token: 168\n",
      "['[UNK]', '[UNK]', 'have', 't', '##ho', '##ugh', '##t', 'it', 'w', '##ould', 'be', 'an', 'excel', '##lent', 'ru', '##le', 'to', 'live', 'ea', '##ch', 'day', 'as', 'if', 'we', 'sh', '##ould', 'die', 'tom', '##or', '##row', '.', '[UNK]', 'an', 'at', '##ti', '##tu', '##de', 'w', '##ould', 'em', '##ph', '##as', '##ize', 'sh', '##ar', '##pl', '##y', 'the', 'value', '##s', 'of', 'life', '.', '[UNK]', 'sh', '##ould', 'live', 'ea', '##ch', 'day', 'with', 'ge', '##nt', '##len', '##ess', ',', 'vi', '##go', '##r', 'and', 'a', 'k', '##een', '##ness', 'of', 'app', '##re', '##cia', '##tion', 'w', '##hi', '##ch', 'are', 'of', '##ten', 'lost', 'when', 'time', 'st', '##re', '##tch', '##es', 'before', 'us', 'in', 'the', 'con', '##sta', '##nt', 'pan', '##ora', '##ma', 'of', 'more', 'days', 'and', 'months', 'and', 'years', 'to', 'come', '.', '[UNK]', 'are', 't', '##ho', '##se', ',', 'of', 'course', ',', 'who', 'w', '##ould', 'ad', '##op', '##t', 'the', '[UNK]', 'mo', '##tto', 'of', '[UNK]', '[UNK]', ',', 'dr', '##ink', ',', 'and', 'be', 'me', '##rry', '[UNK]', '.', '[UNK]', 'most', 'people', 'w', '##ould', 'be', 'ch', '##ast', '##ene', '##d', 'by', 'the', 'ce', '##rt', '##ain', '##ty', 'of', 'im', '##pe', '##ndi', '##ng', 'de', '##ath', '.']\n",
      "num token: 19\n",
      "['这', '是', '一', '段', '中', '文', '语', '料', '，', '我', '希', '望', '它', '能', '够', '被', '分', '词', '。']\n",
      "num token: 64\n",
      "['1', '、', '如', '是', '本', '人', '使', '用', '的', '号', '码', '，', '可', '凭', '户', '主', '有', '效', '证', '件', '到', '营', '业', '厅', '查', '询', '实', '名', '信', '息', '；', '2', '、', '如', '不', '是', '本', '人', '的', '号', '码', '，', '因', '涉', '及', '到', '个', '人', '隐', '私', '，', '无', '法', '查', '询', '到', '户', '主', '的', '个', '人', '信', '息', '。']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "for sentence in corpus:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(f\"num token: {len(tokens)}\")\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "num token: 11\n",
      "['▁It', \"'\", 's', '▁my', '▁great', '▁pleasure', '▁to', '▁be', '▁here', '▁today', '.']\n",
      "num token: 133\n",
      "['▁', '<n>', '<|blank_4|>', 'Sometimes', '▁I', '▁have', '▁thought', '▁it', '▁would', '▁be', '▁an', '▁excellent', '▁rule', '▁to', '▁live', '▁each', '▁day', '▁as', '▁if', '▁we', '▁should', '▁die', '▁tomorrow', '.', '▁', '<n>', '<|blank_4|>', 'Such', '▁an', '▁attitude', '▁would', '▁emphasize', '▁sharply', '▁the', '▁values', '▁of', '▁life', '.', '▁We', '▁should', '▁live', '▁each', '▁day', '▁with', '▁gentle', 'ness', ',', '▁', '<n>', '<|blank_4|>', 'vig', 'or', '▁and', '▁a', '▁keen', 'ness', '▁of', '▁appreciation', '▁which', '▁are', '▁often', '▁lost', '▁when', '▁time', '▁stretches', '▁before', '▁us', '▁in', '▁the', '▁constant', '▁', '<n>', '<|blank_4|>', 'pan', 'orama', '▁of', '▁more', '▁days', '▁and', '▁months', '▁and', '▁years', '▁to', '▁come', '.', '▁There', '▁are', '▁those', ',', '▁of', '▁course', ',', '▁', '<n>', '<|blank_4|>', 'who', '▁would', '▁adopt', '▁the', '▁Epic', 'urea', 'n', '▁motto', '▁of', '▁“', 'Eat', ',', '▁drink', ',', '▁and', '▁be', '▁merry', '”.', '▁', '<n>', '<|blank_4|>', 'But', '▁most', '▁people', '▁would', '▁be', '▁cha', 'sten', 'ed', '▁by', '▁the', '▁certainty', '▁of', '▁impending', '▁death', '.', '<n>', '<|blank_4|>']\n",
      "num token: 12\n",
      "['▁这是', '一段', '中文', '语', '料', ',', '我希望', '它能够', '被', '分', '词', '。']\n",
      "num token: 38\n",
      "['▁', '1', '、', '如是', '本人', '使用的', '号码', ',', '可', '凭', '户', '主', '有效证件', '到', '营业厅', '查询', '实名', '信息', ';', '2', '、', '如', '不是', '本人的', '号码', ',', '因', '涉及到', '个人隐私', ',', '无法', '查询', '到', '户', '主', '的', '个人信息', '。']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "for sentence in corpus:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(f\"num token: {len(tokens)}\")\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num token: 10\n",
      "[b'It', b\"'s\", b' my', b' great', b' pleasure', b' to', b' be', b' here', b' today', b'.']\n",
      "num token: 123\n",
      "[b'\\n', b'   ', b' Sometimes', b' I', b' have', b' thought', b' it', b' would', b' be', b' an', b' excellent', b' rule', b' to', b' live', b' each', b' day', b' as', b' if', b' we', b' should', b' die', b' tomorrow', b'.', b' \\n', b'   ', b' Such', b' an', b' attitude', b' would', b' emphasize', b' sharply', b' the', b' values', b' of', b' life', b'.', b' We', b' should', b' live', b' each', b' day', b' with', b' gent', b'leness', b',', b' \\n', b'   ', b' vigor', b' and', b' a', b' keen', b'ness', b' of', b' appreciation', b' which', b' are', b' often', b' lost', b' when', b' time', b' stretches', b' before', b' us', b' in', b' the', b' constant', b' \\n', b'   ', b' panorama', b' of', b' more', b' days', b' and', b' months', b' and', b' years', b' to', b' come', b'.', b' There', b' are', b' those', b',', b' of', b' course', b',', b' \\n', b'   ', b' who', b' would', b' adopt', b' the', b' Epic', b'ure', b'an', b' motto', b' of', b' \\xe2\\x80\\x9c', b'Eat', b',', b' drink', b',', b' and', b' be', b' merry', b'\\xe2\\x80\\x9d.', b' \\n', b'   ', b' But', b' most', b' people', b' would', b' be', b' chast', b'ened', b' by', b' the', b' certainty', b' of', b' impending', b' death', b'.\\n', b'    ']\n",
      "num token: 25\n",
      "[b'\\xe8\\xbf\\x99', b'\\xe6\\x98\\xaf', b'\\xe4\\xb8\\x80', b'\\xe6\\xae\\xb5', b'\\xe4\\xb8\\xad', b'\\xe6\\x96\\x87', b'\\xe8\\xaf\\xad', b'\\xe6\\x96\\x99', b'\\xef\\xbc\\x8c', b'\\xe6\\x88\\x91', b'\\xe5\\xb8', b'\\x8c', b'\\xe6\\x9c', b'\\x9b', b'\\xe5\\xae', b'\\x83', b'\\xe8\\x83\\xbd', b'\\xe5\\xa4', b'\\x9f', b'\\xe8\\xa2', b'\\xab', b'\\xe5\\x88\\x86', b'\\xe8\\xaf', b'\\x8d', b'\\xe3\\x80\\x82']\n",
      "num token: 63\n",
      "[b'1', b'\\xe3\\x80\\x81', b'\\xe5\\xa6\\x82', b'\\xe6\\x98\\xaf', b'\\xe6\\x9c\\xac', b'\\xe4\\xba\\xba', b'\\xe4\\xbd\\xbf\\xe7\\x94\\xa8', b'\\xe7\\x9a\\x84', b'\\xe5\\x8f\\xb7', b'\\xe7\\xa0\\x81', b'\\xef\\xbc\\x8c', b'\\xe5\\x8f\\xaf', b'\\xe5\\x87', b'\\xad', b'\\xe6\\x88\\xb7', b'\\xe4\\xb8\\xbb', b'\\xe6\\x9c\\x89\\xe6\\x95\\x88', b'\\xe8\\xaf\\x81', b'\\xe4\\xbb\\xb6', b'\\xe5\\x88\\xb0', b'\\xe8\\x90', b'\\xa5', b'\\xe4\\xb8\\x9a', b'\\xe5\\x8e', b'\\x85', b'\\xe6\\x9f\\xa5\\xe8\\xaf\\xa2', b'\\xe5\\xae\\x9e', b'\\xe5\\x90\\x8d', b'\\xe4\\xbf\\xa1\\xe6\\x81\\xaf', b'\\xef\\xbc\\x9b', b'2', b'\\xe3\\x80\\x81', b'\\xe5\\xa6\\x82', b'\\xe4\\xb8\\x8d', b'\\xe6\\x98\\xaf', b'\\xe6\\x9c\\xac', b'\\xe4\\xba\\xba', b'\\xe7\\x9a\\x84', b'\\xe5\\x8f\\xb7', b'\\xe7\\xa0\\x81', b'\\xef\\xbc\\x8c', b'\\xe5\\x9b\\xa0', b'\\xe6\\xb6', b'\\x89', b'\\xe5\\x8f\\x8a', b'\\xe5\\x88\\xb0', b'\\xe4\\xb8\\xaa', b'\\xe4\\xba\\xba', b'\\xe9\\x9a', b'\\x90', b'\\xe7\\xa7\\x81', b'\\xef\\xbc\\x8c', b'\\xe6\\x97\\xa0', b'\\xe6\\xb3\\x95', b'\\xe6\\x9f\\xa5\\xe8\\xaf\\xa2', b'\\xe5\\x88\\xb0', b'\\xe6\\x88\\xb7', b'\\xe4\\xb8\\xbb', b'\\xe7\\x9a\\x84', b'\\xe4\\xb8\\xaa', b'\\xe4\\xba\\xba', b'\\xe4\\xbf\\xa1\\xe6\\x81\\xaf', b'\\xe3\\x80\\x82']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "for sentence in corpus:\n",
    "    token_integers = enc.encode(sentence)\n",
    "    # tokens = enc.decode_bytes(token_integers)\n",
    "    # tokens = [enc.decode_single_token_bytes(token) for token in token_integers]\n",
    "    tokens = enc.decode_tokens_bytes(token_integers)\n",
    "    print(f\"num token: {len(token_integers)}\")\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'\", 's', ' ', 'my', ' ', 'great', ' ', 'pleasure', ' ', 'to', ' ', 'be', ' ', 'here', ' ', 'today', '.']\n",
      "['\\n', ' ', ' ', ' ', ' ', 'Sometimes', ' ', 'I', ' ', 'have', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'be', ' ', 'an', ' ', 'excellent', ' ', 'rule', ' ', 'to', ' ', 'live', ' ', 'each', ' ', 'day', ' ', 'as', ' ', 'if', ' ', 'we', ' ', 'should', ' ', 'die', ' ', 'tomorrow', '.', ' ', '\\n', ' ', ' ', ' ', ' ', 'Such', ' ', 'an', ' ', 'attitude', ' ', 'would', ' ', 'emphasize', ' ', 'sharply', ' ', 'the', ' ', 'values', ' ', 'of', ' ', 'life', '.', ' ', 'We', ' ', 'should', ' ', 'live', ' ', 'each', ' ', 'day', ' ', 'with', ' ', 'gentleness', ',', ' ', '\\n', ' ', ' ', ' ', ' ', 'vigor', ' ', 'and', ' ', 'a', ' ', 'keenness', ' ', 'of', ' ', 'appreciation', ' ', 'which', ' ', 'are', ' ', 'often', ' ', 'lost', ' ', 'when', ' ', 'time', ' ', 'stretches', ' ', 'before', ' ', 'us', ' ', 'in', ' ', 'the', ' ', 'constant', ' ', '\\n', ' ', ' ', ' ', ' ', 'panorama', ' ', 'of', ' ', 'more', ' ', 'days', ' ', 'and', ' ', 'months', ' ', 'and', ' ', 'years', ' ', 'to', ' ', 'come', '.', ' ', 'There', ' ', 'are', ' ', 'those', ',', ' ', 'of', ' ', 'course', ',', ' ', '\\n', ' ', ' ', ' ', ' ', 'who', ' ', 'would', ' ', 'adopt', ' ', 'the', ' ', 'Epicurean', ' ', 'motto', ' ', 'of', ' ', '“', 'Eat', ',', ' ', 'drink', ',', ' ', 'and', ' ', 'be', ' ', 'merry', '”', '.', ' ', '\\n', ' ', ' ', ' ', ' ', 'But', ' ', 'most', ' ', 'people', ' ', 'would', ' ', 'be', ' ', 'chastened', ' ', 'by', ' ', 'the', ' ', 'certainty', ' ', 'of', ' ', 'impending', ' ', 'death', '.', '\\n', ' ', ' ', ' ', ' ']\n",
      "['这是', '一段', '中文', '语料', '，', '我', '希望', '它', '能够', '被', '分词', '。']\n",
      "['1', '、', '如是', '本人', '使用', '的', '号码', '，', '可', '凭', '户主', '有效证件', '到', '营业厅', '查询', '实名', '信息', '；', '2', '、', '如', '不是', '本人', '的', '号码', '，', '因', '涉及', '到', '个人隐私', '，', '无法', '查询', '到', '户主', '的', '个人信息', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "for sentence in corpus:\n",
    "    # tokens = jieba.lcut(sentence, cut_all=False)\n",
    "    tokens = jieba.lcut(sentence, use_paddle=True)\n",
    "    print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare time consumption for Chinese tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000431\n"
     ]
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "这是一段很长的中文文本，用于比较分词耗时。\n",
    "\"\"\" * int(1000000 / 22.99)\n",
    "\n",
    "print(len(long_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num token: 1087426\n",
      "tiktoken: 0.3211658000946045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913437 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num token: 913437\n",
      "bert-base-chinese: 2.6363773345947266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "num token: 608959\n",
      "THUDM/chatglm-6b: 1.2231402397155762\n",
      "num token: 608958\n",
      "jieba: 3.8096296787261963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "start = time.time()\n",
    "token_integers = enc.encode(long_text)\n",
    "print(f\"num token: {len(token_integers)}\")\n",
    "print(f\"tiktoken: {time.time() - start}\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "start = time.time()\n",
    "tokens = tokenizer.tokenize(long_text)\n",
    "print(f\"num token: {len(tokens)}\")\n",
    "print(f\"bert-base-chinese: {time.time() - start}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "start = time.time()\n",
    "tokens = tokenizer.tokenize(long_text)\n",
    "print(f\"num token: {len(tokens)}\")\n",
    "print(f\"THUDM/chatglm-6b: {time.time() - start}\")\n",
    "\n",
    "import jieba\n",
    "start = time.time()\n",
    "tokens = jieba.lcut(long_text)\n",
    "print(f\"num token: {len(tokens)}\")\n",
    "print(f\"jieba: {time.time() - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
