{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ChatGPT to evaluate generated answers in the LLM-as-a-judge way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from gpt_score.gpt3_score import gpt3score\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "proxy = 'http://dell-1.star:7890' # 3090 docker\n",
    "os.environ['http_proxy'] = proxy \n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy\n",
    "\n",
    "openai.api_key_path = \".openai-key2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = {\n",
    "    \"role\": \"system\",\n",
    "    \"prompt\": \"你是一个有用的助手\"\n",
    "}\n",
    "\n",
    "ANSWER_PROMPT = \"\"\"[指令]\n",
    "请充当一个公正的裁判，评估AI助手对下面显示的问题的回答质量。你的评估应该考虑回复的有用性、相关性、准确性、深度、创造力和详细程度等因素。请通过提供简短的解释来开始你的评估，并尽可能做到客观。提供解释后，你必须遵循以下格式对回复进行评分(从1到10)：\\\"[[评分]]\\\"，例如：\\\"评分：[[5]]\\\"。\n",
    "\n",
    "[问题]\n",
    "{question}\n",
    "\n",
    "[AI回答开始]\n",
    "{answer}\n",
    "[AI回答结束]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def eval_questions_by_chat(row, max_tokens=1000):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": ANSWER_PROMPT.format(\n",
    "                        question=row[\"question\"],\n",
    "                        answer=row[\"answer\"]\n",
    "                    )\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\\n\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 84/1076 [03:06<37:45,  2.28s/it]ERROR:root:The server is overloaded or not ready yet.\n",
      " 11%|█         | 117/1076 [04:54<37:37,  2.35s/it] WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v1/chat/completions\n",
      " 30%|██▉       | 319/1076 [14:12<28:11,  2.23s/it]  ERROR:root:The server is overloaded or not ready yet.\n",
      " 55%|█████▌    | 595/1076 [24:48<20:12,  2.52s/it]  ERROR:root:The server is overloaded or not ready yet.\n",
      " 74%|███████▍  | 795/1076 [33:06<09:19,  1.99s/it]  WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v1/chat/completions\n",
      "100%|██████████| 1076/1076 [46:53<00:00,  2.61s/it] \n"
     ]
    }
   ],
   "source": [
    "filename = \"test/manual-qa.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df.rename(columns={\"prompt\": \"question\", \"completion\": \"answer\"}, inplace=True)\n",
    "\n",
    "df[\"eval_raw\"] = df.progress_apply(eval_questions_by_chat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "score_pattern = re.compile(r\"评分：\\[\\[(\\d+)\\]\\]\")\n",
    "\n",
    "df[\"eval_score\"] = df[\"eval_raw\"].apply(lambda x: int(score_pattern.search(x).group(1)) if score_pattern.search(x) else -1)\n",
    "df.to_csv(filename.replace(\".csv\", \"-eval.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df[df[\"eval_score\"] != -1]\n",
    "valid_df.sort_values(by=\"eval_score\", ascending=False) \\\n",
    "    .to_csv(filename.replace(\".csv\", \"-eval-descend.csv\"), index=False)\n",
    "valid_df.sort_values(by=\"eval_score\", ascending=True) \\\n",
    "    .to_csv(filename.replace(\".csv\", \"-eval-ascend.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   eval_score  informative_score  coherent_score  \\\n",
      "eval_score           1.000000           0.088628        0.087818   \n",
      "informative_score    0.088628           1.000000        0.992092   \n",
      "coherent_score       0.087818           0.992092        1.000000   \n",
      "relevant_score       0.081309           0.990572        0.995123   \n",
      "fluent_score         0.087120           0.993130        0.996107   \n",
      "\n",
      "                   relevant_score  fluent_score  \n",
      "eval_score               0.081309      0.087120  \n",
      "informative_score        0.990572      0.993130  \n",
      "coherent_score           0.995123      0.996107  \n",
      "relevant_score           1.000000      0.993676  \n",
      "fluent_score             0.993676      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# calculate correlation scores\n",
    "corr = valid_df[[\"eval_score\", \"informative_score\", \"coherent_score\", \"relevant_score\", \"fluent_score\"]].corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_valid_df = df[df[\"eval_score\"] == -1]\n",
    "non_valid_df.to_csv(filename.replace(\".csv\", \"-eval-non-valid.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
