{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QA in a few-shot learning way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "proxy = 'http://dell-1.star:7890' # 3090 docker\n",
    "# proxy = 'http://127.0.0.1:7890' # clash\n",
    "# proxy = 'http://127.0.0.1:1080' # naiveproxy\n",
    "\n",
    "os.environ['http_proxy'] = proxy \n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy\n",
    "openai.api_key_path = \".openai-key2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"template/question_seeds.json\") as fin:\n",
    "    question_seeds = json.load(fin)\n",
    "with open(\"template/answer_seeds.json\") as fin:\n",
    "    answer_seeds = json.load(fin)\n",
    "\n",
    "QUESTION_PROMPT = \"\"\"请参考如下示例，根据给定文本生成问题，要求尽可能使用简体中文，且表述清晰详细\n",
    "\n",
    "文本：{context_example}\n",
    "\n",
    "问题：\n",
    "{questions_example}\n",
    "\n",
    "文本: {context}\n",
    "\n",
    "问题:\n",
    "1.\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_PROMPT = \"\"\"请参考如下示例，根据给定文本生成问题的答案，要求尽可能使用简体中文，且从文本中找不到答案时回答“无法确定”\n",
    "\n",
    "文本: {context_example}\n",
    "\n",
    "问题:\n",
    "{questions_example}\n",
    "\n",
    "答案:\n",
    "{answers_example}\n",
    "\n",
    "文本: {context}\n",
    "\n",
    "问题:\n",
    "{questions}\n",
    "\n",
    "答案:\n",
    "1.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def gen_questions_by_chat(row, max_tokens=1000):\n",
    "    example = random.choice(question_seeds)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": QUESTION_PROMPT.format(\n",
    "                        context_example=example['context'],\n",
    "                        questions_example=example['questions'],\n",
    "                        context=row.context)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\\n\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def gen_answers_by_chat(row, max_tokens=1000):\n",
    "    example = random.choice(answer_seeds)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": ANSWER_PROMPT.format(\n",
    "                        context_example=example['context'],\n",
    "                        questions_example=example['questions'],\n",
    "                        answers_example=example['answers'],\n",
    "                        context=row.context, \n",
    "                        questions=row.questions)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        return \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "filename = \"book/few-shot/book.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "if \"context\" not in df.columns:\n",
    "    assert \"summary\" in df.columns and \"content\" in df.columns,\\\n",
    "        \"Either 'context' or 'summary' and 'content' must be in the csv file\"\n",
    "    df[\"context\"] = \"summary: \" + df[\"summary\"] + \"\\ncontent: \" + df[\"content\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 26/588 [01:41<35:27,  3.79s/it]  ERROR:root:This model's maximum context length is 4097 tokens. However, your messages resulted in 6359 tokens. Please reduce the length of the messages.\n",
      "  8%|▊         | 49/588 [03:40<47:19,  5.27s/it]  ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4408 tokens (3408 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      " 14%|█▍        | 83/588 [06:29<32:57,  3.92s/it]  WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v1/chat/completions\n",
      " 15%|█▍        | 87/588 [08:23<1:54:41, 13.73s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v1/chat/completions\n",
      " 51%|█████     | 301/588 [20:18<12:56,  2.70s/it]  ERROR:root:The server is overloaded or not ready yet.\n",
      " 69%|██████▊   | 403/588 [25:29<07:43,  2.51s/it]ERROR:root:The server is overloaded or not ready yet.\n",
      " 92%|█████████▏| 543/588 [32:30<02:45,  3.69s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, your messages resulted in 6429 tokens. Please reduce the length of the messages.\n",
      " 95%|█████████▍| 558/588 [33:32<01:56,  3.89s/it]ERROR:root:The server is overloaded or not ready yet.\n",
      " 96%|█████████▋| 566/588 [34:29<01:39,  4.54s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4502 tokens (3502 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "100%|██████████| 588/588 [36:36<00:00,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "df[\"questions\"] = df.progress_apply(gen_questions_by_chat, axis=1)\n",
    "df[\"questions\"] = \"1.\" + df.questions\n",
    "\n",
    "df.to_csv(filename.replace(\".csv\", \"-questions.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 26/588 [04:38<1:12:05,  7.70s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, your messages resulted in 6412 tokens. Please reduce the length of the messages.\n",
      "  8%|▊         | 49/588 [08:31<1:04:49,  7.22s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4779 tokens (3779 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      " 10%|▉         | 56/588 [09:26<49:03,  5.53s/it]  ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4572 tokens (3572 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      " 44%|████▎     | 257/588 [27:41<30:23,  5.51s/it]  WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v1/chat/completions\n",
      " 58%|█████▊    | 343/588 [37:33<21:39,  5.30s/it]  ERROR:root:The server is overloaded or not ready yet.\n",
      " 59%|█████▉    | 348/588 [38:26<29:29,  7.37s/it]ERROR:root:The server is overloaded or not ready yet.\n",
      " 92%|█████████▏| 543/588 [58:16<04:43,  6.31s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, your messages resulted in 6584 tokens. Please reduce the length of the messages.\n",
      " 93%|█████████▎| 549/588 [59:31<07:33, 11.62s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4330 tokens (3330 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      " 96%|█████████▋| 566/588 [1:02:02<03:19,  9.08s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4634 tokens (3634 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      " 97%|█████████▋| 573/588 [1:02:50<01:27,  5.86s/it]ERROR:root:This model's maximum context length is 4097 tokens. However, you requested 4606 tokens (3606 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.\n",
      "100%|██████████| 588/588 [1:05:57<00:00,  6.73s/it]\n"
     ]
    }
   ],
   "source": [
    "df[\"answers\"] = df.progress_apply(gen_answers_by_chat, axis=1)\n",
    "df[\"answers\"] = \"1.\" + df.answers\n",
    "\n",
    "df.to_csv(filename.replace(\".csv\", \"-qa-raw.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split and filter qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filter:  (1840, 2)\n",
      "after length filter:  (1637, 2)\n",
      "after question mark filter:  (1608, 2)\n",
      "after period filter:  (1562, 2)\n",
      "after key word filter:  (1553, 2)\n",
      "after duplicate filter:  (1533, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1533/1533 [00:24<00:00, 63.69it/s]\n",
      "100%|██████████| 1533/1533 [00:00<00:00, 2115.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after similarity filter:  (1139, 4)\n",
      "after rouge filter:  (1115, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'book/few-shot/book-qa-filtered.jsonl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qa_evaluator import split_qa, filter_qa\n",
    "\n",
    "save_filename = split_qa(filename.replace(\".csv\", \"-qa-raw.csv\"))\n",
    "filter_qa(save_filename, output_format=\"jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
